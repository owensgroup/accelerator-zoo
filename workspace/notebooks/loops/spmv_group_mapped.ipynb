{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fa06aa",
   "metadata": {},
   "source": [
    "# TeAAL specifications on variants of Loops' SpMV implementations (Group-Mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ee46",
   "metadata": {},
   "source": [
    "Description: Assign an equal amount of work tiles to a group of threads (warp or blocks). Threads within each group process individual work items in parallel.\n",
    "\n",
    "In this version, each `tile` represents a row of $A$, and each `atom` represents a nonzero entry of $A$. In other words, assigning a thread block to each row of $A$.\n",
    "\n",
    "Template: https://github.com/gunrock/loops/blob/main/include/loops/algorithms/spmv/group_mapped.cuh\n",
    "\n",
    "Scheduler (referenced as `config` below): https://github.com/gunrock/loops/blob/main/include/loops/schedule/group_mapped.hxx\n",
    "\n",
    "GPU Kernel Template (Use it as a reference, don't execute it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55b95f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void __launch_bounds__(threads_per_block, 2) __group_mapped(...) {\n",
    "  # Initialize storage and schedule.\n",
    "  using setup_t = schedule::block_mapped<threads_per_block, index_t, offset_t>; # Using block_mapped, assigning an entire tile (row of A) to a thread block\n",
    "  using storage_t = typename setup_t::storage_t;\n",
    "  __shared__ storage_t temporary_storage;\n",
    "\n",
    "  # Construct the schedule.\n",
    "  setup_t config(temporary_storage, offsets, A_rows, A_nnz);\n",
    "  auto p = config.partition(); # Assigns work tiles to each thread block\n",
    "\n",
    "  for (auto virtual_atom : config.atom_accessor(p)) { # Loop over total work, each thread processing individual work items\n",
    "    auto virtual_tile = config.tile_accessor(virtual_atom, p);\n",
    "\n",
    "    if (!(config.is_valid_accessor(virtual_tile, p)))\n",
    "      continue;\n",
    "\n",
    "    auto A_row = config.tile_id(virtual_tile, p); # Perform a binary-search to find the tile index.\n",
    "\n",
    "    auto A_nz_idx = config.atom_id(virtual_atom, A_row, virtual_tile, p);\n",
    "    atomicAdd(&(Z[A_row]), A_values[A_nz_idx] * B[indices[A_nz_idx]]);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2986d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b113dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiber boilerplate\n",
    "\n",
    "from fibertree_bootstrap import *\n",
    "\n",
    "fibertree_bootstrap(style=\"tree\", animation='movie')\n",
    "\n",
    "# Compilation boilerplate\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a14b2",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initialize the input tensors.\n",
    "\n",
    "For simplicity, the size of a thread warp is the same as the size of a thread block (`WARP_SIZE = BLOCK_SIZE`). Suppose that each GPU SM processes 1 thread warp/block per cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 8\n",
    "J = 8\n",
    "\n",
    "# Hardware Specification\n",
    "NUM_SM = 2 # Number of GPU SMs \n",
    "WARP_SIZE = 2 # Number of threads per warp\n",
    "NUM_THREADS = NUM_SM * WARP_SIZE # Total number of threads\n",
    "\n",
    "print(f\"Hardware Specification\\n  NUM_SM: {NUM_SM}, WARP_SIZE: {WARP_SIZE}, NUM_THREADS: {NUM_THREADS}\")\n",
    "\n",
    "seed = 1\n",
    "\n",
    "A_IJ = Tensor.fromRandom(rank_ids=[\"I\", \"J\"], shape=[I, J], seed=seed, density=[0.9, 0.6], name=\"A\")\n",
    "B_J = Tensor.fromRandom(rank_ids=[\"J\"], shape=[J], seed=seed + 1, density=[1], name=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1afb77",
   "metadata": {},
   "source": [
    "## TeAAL Specifications\n",
    "\n",
    "Rows of matrix $A$ are partitioned across the SMs' warp/block. A thread warp/block can be assigned to a row with all zeros. \n",
    "\n",
    "Note that the current TeAAL specificaiton does not allow to specify the rank of `opt: slip`. This means there exists a synchronization across the SMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1879d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = \"\"\"\n",
    "einsum:\n",
    "  declaration:\n",
    "    A: [I, J]\n",
    "    B: [J]\n",
    "    Z: [I]\n",
    "  expressions:\n",
    "    - Z[i] = A[i, j] * B[j]\n",
    "mapping:\n",
    "  rank-order:\n",
    "    A: [I, J]\n",
    "    B: [J]\n",
    "    Z: [I]\n",
    "  partitioning:\n",
    "    Z:\n",
    "      I: [uniform_shape(GRID_SIZE)]\n",
    "      J: [uniform_occupancy(A.BLOCK_SIZE)]\n",
    "  loop-order:\n",
    "    Z: [I1, I0, J1, J0] \n",
    "    # I1: Number of partitioned rows of A\n",
    "    # I0: Size of each partitioned row of A = GRID_SIZE\n",
    "    # J1: Number of partitioned nonzero elements for a given row\n",
    "    # J0: Size of each partitioned nonzero elements = BLOCK_SIZE (Can be less than BLOCK_SIZE if there are less than BLOCK_SIZE nonzero elements left for the current partition) \n",
    "  spacetime:\n",
    "    Z:\n",
    "      space: [I0, J0]\n",
    "      time: [I1, J1]\n",
    "      # opt: slip # Turning off since currently not working as intended. Refer to the note above.\n",
    "\"\"\"\n",
    "\n",
    "utils.compile(yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231c0d",
   "metadata": {},
   "source": [
    "## Check Results (Correctness)\n",
    "\n",
    "Check that generated code computes the correct result.\n",
    "\n",
    "**Note**: Should be used after compiling and running the kernel (above cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_matrix_vector_mul(A_IJ, B_J, Z_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe9e8d-0678-4246-902a-5fb455beae19",
   "metadata": {},
   "source": [
    "## Performance on GPU\n",
    "\n",
    "Load Balance: Better load balance than Thread-Mapped, since rows with high NNZ will be processed by a group of threads (smoothing out heavy rows). Additionally, there's no need to worry about load balance across the warps since SMs can simply start processing on another thread warp when one warp finishes earlier than the others.\n",
    "\n",
    "Assuming that the $A$ is stored in CSR format, $B$ and $Z$ are in uncompressed vectors, the memory access pattern would be:\n",
    "- A: Coalesced access, threads in a warp are accessing the same row of $A$.\n",
    "- B: Depends on the column indices of each nonzero entry of $A$. The more irregular the sparsity pattern that $A$ has, the more random the column indices of $A$'s nonzero entries will be. This should result in more uncoalesced accesses to $B$.\n",
    "- Z: Coalesced access, threads in a warp are writing the same row of $Z$. At least with writing to $Z$, it may not be as fast as Thread-Mapped since it is done atomically on Group-Mapped to avoid data conflict. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
