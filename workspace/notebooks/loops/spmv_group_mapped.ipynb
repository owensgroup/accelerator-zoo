{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fa06aa",
   "metadata": {},
   "source": [
    "# TeAAL specifications on variants of Loops' SpMV implementations (Group-Mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ee46",
   "metadata": {},
   "source": [
    "Warning: TeALL specs are incomplete, need to make each row partition independent from each other (be able to move onto the next assigned row once done with the current row).\n",
    "\n",
    "Description: Assign an equal amount of work tiles to a group of threads (warp or blocks). Threads within each group process individual work items in parallel.\n",
    "\n",
    "In this version, each `tile` represents a row of $A$, and each `atom` represents a nonzero entry of $A$. In other words, assigning a thread block to each row of $A$.\n",
    "\n",
    "Template: https://github.com/gunrock/loops/blob/main/include/loops/algorithms/spmv/group_mapped.cuh\n",
    "\n",
    "Scheduler (referenced as `config` below): https://github.com/gunrock/loops/blob/main/include/loops/schedule/group_mapped.hxx\n",
    "\n",
    "GPU Kernel Template (Use it as a reference, don't execute it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55b95f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "template <std::size_t threads_per_block,\n",
    "          typename index_t,\n",
    "          typename offset_t,\n",
    "          typename type_t>\n",
    "__global__ void __launch_bounds__(threads_per_block, 2)\n",
    "    __group_mapped(std::size_t rows,\n",
    "                   std::size_t cols,\n",
    "                   std::size_t nnz,\n",
    "                   offset_t* offsets,\n",
    "                   index_t* indices,\n",
    "                   const type_t* values,\n",
    "                   const type_t* x,\n",
    "                   type_t* y) {\n",
    "  using setup_t = schedule::block_mapped<threads_per_block, index_t, offset_t>;\n",
    "\n",
    "  /// Allocate temporary storage for the schedule.\n",
    "  using storage_t = typename setup_t::storage_t;\n",
    "  __shared__ storage_t temporary_storage;\n",
    "\n",
    "  /// Construct the schedule.\n",
    "  setup_t config(temporary_storage, offsets, rows, nnz);\n",
    "  auto p = config.partition();\n",
    "\n",
    "  for (auto virtual_atom : config.atom_accessor(p)) {\n",
    "    auto virtual_tile = config.tile_accessor(virtual_atom, p);\n",
    "\n",
    "    if (!(config.is_valid_accessor(virtual_tile, p)))\n",
    "      continue;\n",
    "\n",
    "    auto row = config.tile_id(virtual_tile, p);\n",
    "\n",
    "    auto nz_idx = config.atom_id(virtual_atom, row, virtual_tile, p);\n",
    "    atomicAdd(&(y[row]), values[nz_idx] * x[indices[nz_idx]]);\n",
    "  }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2986d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b113dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiber boilerplate\n",
    "\n",
    "from fibertree_bootstrap import *\n",
    "\n",
    "fibertree_bootstrap(style=\"tree\", animation='movie')\n",
    "\n",
    "# Compilation boilerplate\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a14b2",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initialize the input tensors.\n",
    "\n",
    "For simplicity, suppose that each GPU SM processes 1 thread warp/block with size `BLOCK_SIZE` per cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 4\n",
    "J = 4\n",
    "\n",
    "NUM_SM = 2 # Number of GPU SMs \n",
    "BLOCK_SIZE = 2 # Number of threads per block \n",
    "NUM_THREADS_PER_CYCLE = BLOCK_SIZE * NUM_SM # Total number of threads processed per cycle\n",
    "\n",
    "print(f\"NUM_SM: {NUM_SM}, BLOCK_SIZE: {BLOCK_SIZE}, NUM_THREADS_PER_CYCLE: {NUM_THREADS_PER_CYCLE}\")\n",
    "seed = 1\n",
    "\n",
    "A_IJ = Tensor.fromRandom(rank_ids=[\"I\", \"J\"], shape=[I, J], seed=seed, density=[0.9, 0.6], name=\"A\")\n",
    "#A_IJ = Tensor.fromUncompressed(rank_ids=[\"I\", \"J\"], shape=[I, J], root=[[1],[2],[0],[4]], name=\"A\")\n",
    "B_J = Tensor.fromRandom(rank_ids=[\"J\"], shape=[J], seed=seed + 1, density=[1], name=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1afb77",
   "metadata": {},
   "source": [
    "TeAAL Specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1879d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = \"\"\"\n",
    "einsum:\n",
    "  declaration:\n",
    "    A: [I, J]\n",
    "    B: [J]\n",
    "    Z: [I]\n",
    "  expressions:\n",
    "    - Z[i] = A[i, j] * B[j]\n",
    "mapping:\n",
    "  rank-order:\n",
    "    A: [I, J]\n",
    "    B: [J]\n",
    "    Z: [I]\n",
    "  partitioning:\n",
    "    Z:\n",
    "      I: [uniform_shape(NUM_SM)]\n",
    "      J: [uniform_occupancy(A.BLOCK_SIZE)]\n",
    "  loop-order:\n",
    "    Z: [I1, I0, J1, J0] \n",
    "  spacetime:\n",
    "    Z:\n",
    "      space: [I0, J0]\n",
    "      time: [I1, J1]\n",
    "      opt: slip\n",
    "\"\"\"\n",
    "\n",
    "utils.compile(yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231c0d",
   "metadata": {},
   "source": [
    "## Check Results\n",
    "\n",
    "Check that generated code computes the correct result.\n",
    "\n",
    "**Note**: Should be used after compiling and running the kernel (above cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_matrix_vector_mul(A_IJ, B_J, Z_I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
