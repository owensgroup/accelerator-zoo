{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fa06aa",
   "metadata": {},
   "source": [
    "# TeAAL specifications on variants of Loops' SpMV implementations (Group-Mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ee46",
   "metadata": {},
   "source": [
    "Description: Assign an equal amount of work tiles to a group of threads (warp or blocks). Threads within each group process individual work items in parallel.\n",
    "\n",
    "In this version, each `tile` represents a row of $A$, and each `atom` represents a nonzero entry of $A$. In other words, assigning a thread block to each row of $A$.\n",
    "\n",
    "Template: https://github.com/gunrock/loops/blob/main/include/loops/algorithms/spmv/group_mapped.cuh\n",
    "\n",
    "Scheduler (referenced as `config` below): https://github.com/gunrock/loops/blob/main/include/loops/schedule/group_mapped.hxx\n",
    "\n",
    "GPU Kernel Template (Use it as a reference, don't execute it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55b95f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void __launch_bounds__(threads_per_block, 2) __group_mapped(...) {\n",
    "  # Initialize storage and schedule.\n",
    "  using setup_t = schedule::block_mapped<threads_per_block, index_t, offset_t>; # Using block_mapped, assigning an entire tile of work to a thread block\n",
    "  using storage_t = typename setup_t::storage_t;\n",
    "  __shared__ storage_t temporary_storage;\n",
    "\n",
    "  # Construct the schedule.\n",
    "  setup_t config(temporary_storage, offsets, rows, nnz);\n",
    "  auto p = config.partition(); # Assigns work tiles to each thread block\n",
    "\n",
    "  for (auto virtual_atom : config.atom_accessor(p)) { # Loop over total work, each thread processing individual work items\n",
    "    auto virtual_tile = config.tile_accessor(virtual_atom, p);\n",
    "\n",
    "    if (!(config.is_valid_accessor(virtual_tile, p)))\n",
    "      continue;\n",
    "\n",
    "    auto row = config.tile_id(virtual_tile, p); # Perform a binary-search to find the tile index.\n",
    "\n",
    "    auto nz_idx = config.atom_id(virtual_atom, row, virtual_tile, p);\n",
    "    atomicAdd(&(y[row]), values[nz_idx] * x[indices[nz_idx]]);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2986d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b113dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bootstrap\n",
      "The fibertree module is already installed and available to import\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7aa2c01be642628251776f916c91c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='style', options=('tree', 'uncompressed', 'tree+uncompressed'), valâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eae3d7da25440aab1ae0b4b1d5c941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run all cells below', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HiFiber boilerplate\n",
    "\n",
    "from fibertree_bootstrap import *\n",
    "\n",
    "fibertree_bootstrap(style=\"tree\", animation='movie')\n",
    "\n",
    "# Compilation boilerplate\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a14b2",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initialize the input tensors.\n",
    "\n",
    "For simplicity, the size of a thread warp is the same as the size of a thread block (`WARP_SIZE = BLOCK_SIZE`). Suppose that each GPU SM processes 1 thread warp/block per cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1769f0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Specification\n",
      "  NUM_SM: 2, WARP_SIZE: 2, NUM_THREADS: 4\n"
     ]
    }
   ],
   "source": [
    "I = 8\n",
    "J = 8\n",
    "\n",
    "# Hardware Specification\n",
    "NUM_SM = 2 # Number of GPU SMs \n",
    "WARP_SIZE = 2 # Number of threads per warp\n",
    "NUM_THREADS = NUM_SM * WARP_SIZE # Total number of threads\n",
    "\n",
    "print(f\"Hardware Specification\\n  NUM_SM: {NUM_SM}, WARP_SIZE: {WARP_SIZE}, NUM_THREADS: {NUM_THREADS}\")\n",
    "\n",
    "seed = 1\n",
    "\n",
    "A_IJ = Tensor.fromRandom(rank_ids=[\"I\", \"J\"], shape=[I, J], seed=seed, density=[0.9, 0.6], name=\"A\")\n",
    "B_J = Tensor.fromRandom(rank_ids=[\"J\"], shape=[J], seed=seed + 1, density=[1], name=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1afb77",
   "metadata": {},
   "source": [
    "## TeAAL Specifications\n",
    "\n",
    "Rows of matrix $A$ are partitioned across the SMs' warp/block. A thread warp/block can be assigned to a row with all zeros. \n",
    "\n",
    "Note that the current TeAAL specificaiton does not allow to specify the rank of `opt: slip`. This means there exists a synchronization across the SMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1879d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = \"\"\"\n",
    "einsum:\n",
    "  declaration:\n",
    "    A: [I, J]\n",
    "    B: [J]\n",
    "    Z: [I]\n",
    "  expressions:\n",
    "    - Z[i] = A[i, j] * B[j]\n",
    "mapping:\n",
    "  rank-order:\n",
    "    A: [I, J]\n",
    "    B: [J]\n",
    "    Z: [I]\n",
    "  partitioning:\n",
    "    Z:\n",
    "      I: [uniform_shape(NUM_SM)]\n",
    "      J: [uniform_occupancy(A.WARP_SIZE)]\n",
    "  loop-order:\n",
    "    Z: [I1, I0, J1, J0] \n",
    "    # I1: Number of partitioned rows (I)\n",
    "    # I0: Size of each partitioned row = NUM_SM\n",
    "    # J1: Number of partitioned nonzero elements for a given row\n",
    "    # J0: Size of each partitioned nonzero elements = WARP_SIZE (Can be less than WARP_SIZE if there are less than WARP_SIZE nonzero elements left for the current partition) \n",
    "  spacetime:\n",
    "    Z:\n",
    "      space: [I0, J0]\n",
    "      time: [I1, J1]\n",
    "      #opt: slip # Currently not working as intended. Refer to the note above.\n",
    "\"\"\"\n",
    "\n",
    "utils.compile(yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3276d97-2e0c-4287-8aaa-5fe5adab5043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting simulation\n",
      "Finished simulation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60f2f5921b2436b80837a7cf9db6fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create individual tensor images for each cycle:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9224a880b3247108144fec3127749ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paste individual tensor images into frame for each cycle:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe4cfe3c0ca42b395dba1928fb8e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render video frame for each cycle:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./tmp/2026.02.15_155823.RymMOmbWMM.mp4\"  loop autoplay controls  width=\"800\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Autogenerated HiFiber\n",
    "\n",
    "Z_I1I0 = Tensor(rank_ids=[\"I1\", \"I0\"], name=\"Z\")\n",
    "tmp0 = A_IJ\n",
    "tmp1 = tmp0.splitUniform(NUM_SM, depth=0)\n",
    "A_I1I0J = tmp1\n",
    "A_I1I0J.setRankIds(rank_ids=[\"I1\", \"I0\", \"J\"])\n",
    "z_i1 = Z_I1I0.getRoot()\n",
    "b_j = B_J.getRoot()\n",
    "a_i1 = A_I1I0J.getRoot()\n",
    "canvas = createCanvas(A_I1I0J, B_J, Z_I1I0)\n",
    "timestamps = {}\n",
    "B_J = Tensor.fromFiber(rank_ids=[\"J\"], fiber=b_j, name=\"B\")\n",
    "for i1, (z_i0, a_i0) in z_i1 << a_i1:\n",
    "    for i0_pos, (i0, (z_ref, a_j)) in enumerate(z_i0 << a_i0):\n",
    "        A_J = Tensor.fromFiber(rank_ids=[\"J\"], fiber=a_j, name=\"A\")\n",
    "        tmp2 = A_J\n",
    "        tmp3 = tmp2.splitEqual(WARP_SIZE)\n",
    "        A_J1J0 = tmp3\n",
    "        A_J1J0.setRankIds(rank_ids=[\"J1\", \"J0\"])\n",
    "        a_j1 = A_J1J0.getRoot()\n",
    "        tmp4 = B_J\n",
    "        tmp5 = tmp4.splitNonUniform(a_j1)\n",
    "        B_J1J0 = tmp5\n",
    "        B_J1J0.setRankIds(rank_ids=[\"J1\", \"J0\"])\n",
    "        b_j1 = B_J1J0.getRoot()\n",
    "        for j1, (a_j0, b_j0) in a_j1 & b_j1:\n",
    "            for j0_pos, (j0, (a_val, b_val)) in enumerate(a_j0 & b_j0):\n",
    "                z_ref += a_val * b_val\n",
    "                if (i0_pos, j0_pos) in timestamps.keys():\n",
    "                    timestamps[(i0_pos, j0_pos)] += 1\n",
    "                else:\n",
    "                    timestamps[(i0_pos, j0_pos)] = 1\n",
    "                canvas.addActivity((i1, i0, j0), (j0,), (i1, i0), spacetime=((i0_pos, j0_pos), (timestamps[(i0_pos, j0_pos)] - 1,)))\n",
    "tmp6 = Z_I1I0\n",
    "tmp7 = tmp6.mergeRanks(depth=0, levels=1, coord_style=\"absolute\")\n",
    "tmp7.setRankIds(rank_ids=[\"I\"])\n",
    "Z_I = tmp7\n",
    "displayCanvas(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231c0d",
   "metadata": {},
   "source": [
    "## Check Results\n",
    "\n",
    "Check that generated code computes the correct result.\n",
    "\n",
    "**Note**: Should be used after compiling and running the kernel (above cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cfd1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result correct? True\n"
     ]
    }
   ],
   "source": [
    "utils.check_matrix_vector_mul(A_IJ, B_J, Z_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57487f5c-97fe-424b-80c6-0ae85365e618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
