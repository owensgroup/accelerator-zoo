{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fa06aa",
   "metadata": {},
   "source": [
    "# TeAAL specifications on variants of Loops' SpGEMM implementations (Thread-Mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ee46",
   "metadata": {},
   "source": [
    "Description: Assign an equal amount of work tiles to a group of threads (warp or blocks). Threads within each group process individual work items in parallel.\n",
    "\n",
    "In this version, each `tile` represents a row of $A$, and each `atom` represents a nonzero entry of $A$. In other words, assigning a thread block to each row of $A$. For SpGEMM, following Gustavson's row-row dataflow (multiply a row of A and a row of B to get a partial row of Z, then merge the partial rows) as instructed in [A Programming Model for GPU Load Balancing](https://arxiv.org/abs/2301.04792)\n",
    "\n",
    "Template: DNE in Loops\n",
    "\n",
    "Scheduler (referenced as `config` below): https://github.com/gunrock/loops/blob/main/include/loops/schedule/group_mapped.hxx\n",
    "\n",
    "GPU Kernel Template (Use it as a reference, don't execute it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55b95f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "outputs": [],
   "source": [
    "__global__ void __launch_bounds__(threads_per_block, 2) __group_mapped(...) {\n",
    "  # Initialize storage and schedule.\n",
    "  using setup_t = schedule::block_mapped<threads_per_block, index_t, offset_t>; # Using block_mapped, assigning an entire tile of work to a thread block\n",
    "  using storage_t = typename setup_t::storage_t;\n",
    "  __shared__ storage_t temporary_storage;\n",
    "\n",
    "  # Construct the schedule.\n",
    "  setup_t config(temporary_storage, offsets, A_rows, A_nnz);\n",
    "  auto p = config.partition(); # Assigns work tiles to each thread block\n",
    "\n",
    "  for (auto virtual_atom : config.atom_accessor(p)) { # Loop over total work, each thread processing individual work items\n",
    "    auto virtual_tile = config.tile_accessor(virtual_atom, p);\n",
    "\n",
    "    if (!(config.is_valid_accessor(virtual_tile, p)))\n",
    "      continue;\n",
    "\n",
    "    auto A_row = config.tile_id(virtual_tile, p); # Perform a binary-search to find the tile index.\n",
    "    auto A_nz_idx = config.atom_id(virtual_atom, A_row, virtual_tile, p);\n",
    "\n",
    "    for (auto B_nz_idx : get_B_nz(get_A_col(A_nz_idx))) { # Loop over nonzero entry of j-th row of B and multiply\n",
    "      B_col = get_B_col(B_nz_idx);\n",
    "      Z(A_row, B_col) += A_values[A_nz_idx] * B_values[B_nz_idx];\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2986d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b113dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiber boilerplate\n",
    "\n",
    "from fibertree_bootstrap import *\n",
    "\n",
    "fibertree_bootstrap(style=\"tree\", animation='movie')\n",
    "\n",
    "# Compilation boilerplate\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../../\")\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a14b2",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initialize the input tensors.\n",
    "\n",
    "For simplicity, the size of a thread warp is the same as the size of a thread block (`WARP_SIZE = BLOCK_SIZE`). Suppose that each GPU SM processes 1 thread warp/block per cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "M = 4\n",
    "N = 4\n",
    "\n",
    "# Hardware Specification\n",
    "NUM_SM = 2 # Number of GPU SMs \n",
    "WARP_SIZE = 2 # Number of threads per warp\n",
    "NUM_THREADS = NUM_SM * WARP_SIZE # Total number of threads\n",
    "\n",
    "print(f\"Hardware Specification\\n  NUM_SM: {NUM_SM}, WARP_SIZE: {WARP_SIZE}, NUM_THREADS: {NUM_THREADS}\")\n",
    "\n",
    "seed = 1\n",
    "\n",
    "# SpGEMM, both A and B are sparse\n",
    "A_MK = Tensor.fromRandom(rank_ids=[\"M\", \"K\"], shape=[M, K], seed=seed, density=[0.9, 0.6], name=\"A\")\n",
    "B_KN = Tensor.fromRandom(rank_ids=[\"K\", \"N\"], shape=[K, N], seed=seed, density=[0.9, 0.6], name=\"B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1afb77",
   "metadata": {},
   "source": [
    "## TeAAL Specifications\n",
    "\n",
    "Rows of matrix $A$ are partitioned across the SMs' threads. A thread can be assigned to a row with all zeros. \n",
    "\n",
    "Note that the current TeAAL specificaiton does not allow to specify the rank of `opt: slip`. This means there exists a synchronization across the SMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1879d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml = \"\"\"\n",
    "einsum:\n",
    "  declaration:\n",
    "    A: [M, K]\n",
    "    B: [K, N]\n",
    "    Z: [M, N]\n",
    "  expressions:\n",
    "    - Z[m, n] = A[m, k] * B[k, n]\n",
    "mapping:\n",
    "  rank-order:\n",
    "    A: [M, K]\n",
    "    B: [K, N]\n",
    "    Z: [M, N]\n",
    "  partitioning:\n",
    "    Z:\n",
    "      M: [uniform_shape(NUM_SM)]\n",
    "      K: [uniform_occupancy(A.WARP_SIZE)]\n",
    "  loop-order:\n",
    "    Z: [M1, M0, K1, K0, N]\n",
    "    # M1: Number of partitioned rows of A\n",
    "    # M0: Size of each partitioned row of A = NUM_SM\n",
    "    # K1: Number of partitioned nonzero elements for a given row\n",
    "    # K0: Size of each partitioned nonzero elements = WARP_SIZE (Can be less than WARP_SIZE if there are less than WARP_SIZE nonzero elements left for the current partition) \n",
    "  spacetime:\n",
    "    Z:\n",
    "      space: [M0, K0]\n",
    "      time: [M1, K1, N] \n",
    "\"\"\"\n",
    "\n",
    "utils.compile(yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231c0d",
   "metadata": {},
   "source": [
    "## Check Results\n",
    "\n",
    "Check that generated code computes the correct result.\n",
    "\n",
    "**Note**: Should be used after compiling and running the kernel (above cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cfd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.check_matmul(A_MK, B_KN, Z_MN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feccd38-93e7-444c-afa7-b43f0b4d1bb1",
   "metadata": {},
   "source": [
    "## Performance on GPU\n",
    "\n",
    "Load Balance: Better load balance than Thread-Mapped, since rows with high NNZ will be processed by a group of threads (smoothing out heavy rows). Additionally, there's no need to worry about load balance across the warps since SMs can simply start processing on another thread warp when one warp finishes earlier than the others.\n",
    "\n",
    "Assuming that the $A$, $B$, and $Z$ are stored in CSR format, the memory access pattern would be:\n",
    "- A: Coalesced access, threads in a warp are accessing the same row of $A$.\n",
    "- B: Uncoalesced access, threads in a warp are accessing rows of $B$ based on column indices of $A$'s nonzero entries.\n",
    "- Z: Coalesced access, threads in a warp are writing the same row of $Z$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
